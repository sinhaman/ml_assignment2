{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a21bd0c1",
   "metadata": {},
   "source": [
    "1. What is Simple Linear Regression?\n",
    "A statistical method that models the linear relationship between one continuous predictor (X) and one continuous outcome/response (Y):\n",
    "\n",
    "Y=mX+c+Œµ\n",
    "where \n",
    "m is slope, \n",
    "c is intercept, and \n",
    "Œµ is random error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed39993",
   "metadata": {},
   "source": [
    "2. Key assumptions of Simple Linear Regression\n",
    "Linearity: The expected value of Y is a straight‚Äëline function of X.\n",
    "\n",
    "Independence of errors: Residuals are independent across observations.\n",
    "\n",
    "Homoscedasticity: Constant variance of errors across X.\n",
    "\n",
    "Normality of errors (for inference): Residuals are normally distributed (mainly needed for valid t/F tests & CIs).\n",
    "\n",
    "No high‚Äëleverage outliers distorting the line.\n",
    "\n",
    "X measured without (or with negligible) error in classical OLS settings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c985743",
   "metadata": {},
   "source": [
    "3. What does the coefficient \n",
    "m represent in \n",
    "Y=mX+c?\n",
    "The expected change in Y for a one‚Äëunit increase in X.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393462de",
   "metadata": {},
   "source": [
    "4. What does the intercept \n",
    "c represent in \n",
    "\n",
    "Y=mX+c?\n",
    "The expected value of Y when\n",
    "X=0 (if 0 lies in the data range; otherwise it‚Äôs an extrapolated baseline)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f8da72",
   "metadata": {},
   "source": [
    "5. How do we calculate the slope \n",
    "ùëö\n",
    "m in Simple Linear Regression?\n",
    "Using least squares estimates:\n",
    "\n",
    "m= ‚àë i=1n\n",
    " (x i\n",
    " ‚àí xÀâ\n",
    ") 2\n",
    " ‚àë i=1n\n",
    " (x i\n",
    " ‚àí xÀâ )(y i\n",
    " ‚àí y\n",
    "\n",
    " )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6706df",
   "metadata": {},
   "source": [
    "6. Purpose of the least squares method\n",
    "Choose \n",
    "ùëö\n",
    "m and \n",
    "ùëê\n",
    "c that minimize the sum of squared residuals:\n",
    "m,c\n",
    "min\n",
    "‚Äã\n",
    "  \n",
    "i=1\n",
    "‚àën(yi ‚àí^  =‚àë( ‚àí(m +c)2\n",
    " .\n",
    "This gives the ‚Äúbest‚Äëfitting‚Äù line (in the squared‚Äëerror sense) and leads to nice statistical properti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa249c35",
   "metadata": {},
   "source": [
    "7. How is the coefficient of determination (R¬≤) interpreted?\n",
    "The proportion of the total variation in Y (relative to its mean) that is explained by the regression model.\n",
    "\n",
    "ùëÖ2=1‚àíSSresSStotR \n",
    "‚Äã\n",
    " \n",
    "Ranges 0‚Äì1 (can be negative in some adjusted or forced‚Äëthrough‚Äëorigin contexts); higher means the model reduces more variance compared with using just \n",
    "ùë¶\n",
    "Àâ\n",
    "y\n",
    "Àâ\n",
    "‚Äã\n",
    " .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e1b4f8",
   "metadata": {},
   "source": [
    "8. What is Multiple Linear Regression?\n",
    "Multiple Linear Regression (MLR) is a statistical method used to model the relationship between one dependent variable (Y) and two or more independent variables (X‚ÇÅ, X‚ÇÇ, ‚Ä¶, X‚Çö). It assumes a linear relationship between the dependent variable and the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21fda7",
   "metadata": {},
   "source": [
    "9. Main difference: Simple vs Multiple Linear Regression\n",
    "Simple: One predictor (X).\n",
    "\n",
    "Multiple: Two or more predictors; each coefficient reflects the effect of its predictor holding the others constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fce7c54",
   "metadata": {},
   "source": [
    "10. Key assumptions of Multiple Linear Regression\n",
    "Same core assumptions as simple regression, plus:\n",
    "\n",
    "Linearity & additivity in predictors.\n",
    "\n",
    "Independence of errors.\n",
    "\n",
    "Homoscedasticity of errors.\n",
    "\n",
    "Normality of errors (for inference).\n",
    "\n",
    "No perfect multicollinearity (predictors not exact linear combos).\n",
    "\n",
    "Correct model specification (important omitted variables can bias coefficients)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb659cdf",
   "metadata": {},
   "source": [
    "11. What is heteroscedasticity, and why does it matter?\n",
    "Heteroscedasticity = non‚Äëconstant error variance across fitted values or levels of predictors.\n",
    "Consequences under OLS: coefficient estimates remain unbiased (if other assumptions hold), but standard errors become wrong, leading to unreliable hypothesis tests, confidence intervals, and prediction intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f03026",
   "metadata": {},
   "source": [
    "12. How to improve a model with high multicollinearity\n",
    "Remove or combine highly correlated predictors.\n",
    "\n",
    "Use domain knowledge to pick the most meaningful variable.\n",
    "\n",
    "Center or standardize predictors (helps interpretation; doesn‚Äôt ‚Äúfix‚Äù collinearity but reduces numerical issues).\n",
    "\n",
    "Principal Component Regression (PCR) or Partial Least Squares (PLS).\n",
    "\n",
    "Regularization: Ridge (shrinks correlated coefficients), Lasso (variable selection), Elastic Net (hybrid).\n",
    "\n",
    "Collect more data with greater variation in predictors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e8d16",
   "metadata": {},
   "source": [
    "13. Transforming categorical variables for regression\n",
    "Common encodings:\n",
    "\n",
    "Dummy / One‚ÄëHot Encoding: 0/1 indicator variables (k‚Äë1 dummies for k categories to avoid the dummy trap).\n",
    "\n",
    "Effect / Deviation Coding: Compare each level to the overall mean instead of a baseline level.\n",
    "\n",
    "Ordinal Coding: Use meaningful numeric scores when category order matters.\n",
    "\n",
    "Target / Mean Encoding: Replace each category with mean Y (risk of leakage; use regularization / cross‚Äëfold).\n",
    "\n",
    "Binary encoding / Hashing (high‚Äëcardinality cases).\n",
    "\n",
    "Embeddings (in ML frameworks with large, complex categorical spaces).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6f514",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "14. Role of interaction terms in Multiple Linear Regression\n",
    "Interaction terms (e.g., \n",
    "ùëãX 1\n",
    "\n",
    "√óX 2\n",
    "‚Äã\n",
    " ) let the effect of one predictor depend on the level of another. Without them, the model assumes additive, independent effects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c152425",
   "metadata": {},
   "source": [
    "15. How can the interpretation of the intercept differ: Simple vs Multiple?\n",
    "Simple: Expected Y when X=0.\n",
    "\n",
    "Multiple: Expected Y when all predictors are 0 (and at reference levels for categorical variables). This combination may not be realistic; centering predictors (e.g., subtracting their means) can make the intercept represent the expected Y at ‚Äúaverage‚Äù predictor values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe72187",
   "metadata": {},
   "source": [
    "16. Significance of the slope in regression analysis & its effect on predictions\n",
    "A slope coefficient tells how much Y is expected to change per unit change in that predictor (holding others constant in multiple regression). A statistically significant slope (p‚Äëvalue small; CI not containing 0) suggests the predictor contributes meaningfully to explaining Y.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d44be",
   "metadata": {},
   "source": [
    "17. How does the intercept provide context?\n",
    "It anchors the regression surface: the predicted baseline level of Y when predictors are at their reference (or zero/centered) values. It helps interpret whether predictions for realistic ranges are shifted up or down overall, and is essential for making predictions at any combination of predictors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9489e78",
   "metadata": {},
   "source": [
    "18. Limitations of R¬≤ as a sole performance measure\n",
    "Always (or almost always) increases when adding predictors‚Äîeven noisy ones.\n",
    "\n",
    "Doesn‚Äôt indicate whether coefficients are unbiased or meaningful.\n",
    "\n",
    "Doesn‚Äôt assess model correctness, residual structure, or predictive accuracy on new data.\n",
    "\n",
    "Can be high for non‚Äëcausal associations.\n",
    "\n",
    "Sensitive to outliers and to range of Y.\n",
    "Use with Adjusted R¬≤, RMSE, MAE, cross‚Äëvalidation error, residual diagnostics, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad428340",
   "metadata": {},
   "source": [
    "19. Interpreting a large standard error for a coefficient\n",
    "The estimate of that coefficient is imprecise: the data do not strongly support a specific value. Confidence intervals will be wide, and hypothesis tests may fail to reject 0 even if the point estimate is large. Often caused by multicollinearity, small sample size, or high noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37138329",
   "metadata": {},
   "source": [
    "20. Detecting heteroscedasticity in residual plots & why it matters\n",
    "Visual clues:\n",
    "\n",
    "Residuals vs fitted values show a funnel (widening) shape, bow‚Äëtie, or pattern in spread.\n",
    "\n",
    "Residuals vs a predictor show variance changing with predictor level.\n",
    "\n",
    "Scale‚ÄëLocation (Spread‚ÄëLocation) plot: trend in the square root of |standardized residuals|.\n",
    "\n",
    "Formal tests: Breusch‚ÄìPagan, White, Goldfeld‚ÄìQuandt.\n",
    "\n",
    "Why address it: Inference (SEs, p‚Äëvalues, CIs) becomes unreliable; prediction intervals may be too narrow or too wide. Remedies: use heteroscedasticity‚Äërobust (Huber‚ÄìWhite) standard errors, transform Y, model variance structure (e.g., weighted least squares), or use generalized least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2acbf",
   "metadata": {},
   "source": [
    "21. What does it mean if a Multiple Linear Regression model has a high R¬≤ but low Adjusted R¬≤?\n",
    "High R¬≤: The model explains a large proportion of variance in Y.\n",
    "\n",
    "Low Adjusted R¬≤: Suggests that the increase in R¬≤ is mostly due to adding predictors that do not actually improve model fit significantly. Adjusted R¬≤ penalizes unnecessary complexity, so if it's low relative to R¬≤, the model likely suffers from overfitting or has many irrelevant predictors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803644b",
   "metadata": {},
   "source": [
    "22. Why is it important to scale variables in Multiple Linear Regression?\n",
    "Reason: Different predictors can have different scales (e.g., age in years vs. income in thousands). Scaling:\n",
    "\n",
    "Improves numerical stability during optimization.\n",
    "\n",
    "Makes coefficients comparable in magnitude (for standardized interpretation).\n",
    "\n",
    "Important when using regularization (Ridge, Lasso), because penalties depend on coefficient size, which is influenced by variable scale.\n",
    "\n",
    "Common methods: Standardization (z-score) or Min-Max scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dea81f",
   "metadata": {},
   "source": [
    "23. What is Polynomial Regression?\n",
    "A regression technique where the relationship between X and Y is modeled as an nth-degree polynomial:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3459cd",
   "metadata": {},
   "source": [
    "24. How does polynomial regression differ from linear regression?\n",
    "Linear Regression: Model is linear in predictors (straight line).\n",
    "\n",
    "Polynomial Regression: Adds higher-order terms (X¬≤, X¬≥, ‚Ä¶) to capture non-linear relationships between X and Y.\n",
    "\n",
    "Note: Despite the curve, the model is linear in parameters (Œ≤s), so it's still solved by OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d062bff9",
   "metadata": {},
   "source": [
    "25.When is polynomial regression used?\n",
    "When the relationship between X and Y is non-linear, but can be approximatCan polynomial regression be applied to multiple variables?\n",
    "Yes:\n",
    " +‚Ä¶\n",
    "This is called Multivariate Polynomial Regression. It grows quickly in complexity because of interaction terms.ed by a polynomial curve.\n",
    "\n",
    "Common in physics, growth curves, economics, and when residual plots sugges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8406fc19",
   "metadata": {},
   "source": [
    " 26. Limitations of polynomial regression\n",
    "Overfitting risk: High-degree polynomials fit noise.\n",
    "\n",
    "Extrapolation issues: Predictions outside observed X range can be wildly inaccurate.\n",
    "\n",
    "Multicollinearity: Higher-order terms correlate strongly with each other.\n",
    "\n",
    "Interpretability: Harder as degree increases.\n",
    "\n",
    "Computationally expensive for high-degree and multiple variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f521a",
   "metadata": {},
   "source": [
    "27.Methods to select polynomial degree (evaluate model fit)\n",
    "Cross-validation (CV): k-fold CV to pick the degree with lowest error on validation data.\n",
    "\n",
    "Information Criteria: AIC, BIC penalize complexity.\n",
    "\n",
    "Adjusted R¬≤: Prefers simpler models if extra terms add little value.\n",
    "\n",
    "Validation curves (plot error vs. degree).\n",
    "\n",
    "Regularization: Ridge/Lasso with polynomial features to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6e9e0c",
   "metadata": {},
   "source": [
    "28.Why is visualization important in polynomial regression?\n",
    "Visualization helps detect if:\n",
    "\n",
    "The curve fits data well (not underfitting/overfitting).\n",
    "\n",
    "Residuals show no clear pattern (model adequacy).\n",
    "\n",
    "Scatterplot with fitted curve, residual plots, and learning curves are essential to interpret performance and bias-variance trade-off.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f589b7e0",
   "metadata": {},
   "source": [
    "29. How is polynomial regression implemented in Python?\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Example: Polynomial Regression of degree 3\n",
    "model = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=3)),\n",
    "    ('linear', LinearRegression())\n",
    "])\n",
    "\n",
    "# X must be 2D\n",
    "model.fit(X.reshape(-1, 1), y)\n",
    "y_pred = model.predict(X.reshape(-1, 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed93965",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
